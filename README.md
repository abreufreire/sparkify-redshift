# Data Warehouse with Amazon Redshift
### Project overview
**Sparkify** is a newness music company looking to provide a great service to the community that enjoys its music 
operating system. Its business is expanding on a large scale; the response to this scenario is the infrastructure as 
code (IaC); Sparkify is looking for a shift of the operations to cloud services and data warehousing regarding 
product scalability and flexibility. 

At this point the company stores a collection of complex event data and metadata (JSON format) about all music usage and 
ownership data in Amazon S3.

As data engineers we have been asked to implement and optimize Sparkify data and data pipeline architecture, 
as well as optimizing data flow and collection in order to support data teams (scientists and analysts) needs, 
discovering the insights (i.e. behaviours, interactions) between users and the music.

For this job we have adopted the AWS SDK for Python (Boto3) to create, configure, and manage AWS services; 
we have created Redshift cluster for data warehousing containing staging tables as a staging area between S3 buckets 
and production database (final tables using star-schema). Using Boto3 and SQL we have made the ingestion of data from 
S3 into Redshift, transformed and optimized it, and load into the final database, which is the data source of the 
analytics team.

For the sake of this project the data and metadata under processing is part of the Million Song Dataset<sup>1</sup> - 
the **song dataset** contains metadata about songs and artists. The **log dataset** holds log files about users 
activity and was generated by the program [eventsim](https://github.com/Interana/eventsim).

The tables schemas illustrates how data has been transformed along the pipeline. 

![Logo](https://github.com/abreufreire/sparkify-postgres/blob/master/graphics/logo.png)


### Project files
```
sparkify-postgres
|  .gitignore                   # Config file for Git
|  analytics.ipynb              # Queries to test sparkifydb
|  clean_redshift.py            # Cleans AWS services
|  create_tables.py             # Creates staging & production tables using sql_queries.py
|  dwh.cfg                      # Configuration parameters for AWS
|  etl.py                       # Runs ETL pipeline to ingest & load data using sql_queries.py
|  README.md                    # Repository description
|  requirements.txt             # Contains libraries needed to run scripts
|  setup_cluster.py             # Launches AWS services
|  sql_queries.py               # Defines queries to drop & create tables & to copy/ingest & insert/load data
|
â””--graphics
  |  logo.png                   # Data warehouse logo
  |  stag_prod_tables.png       # Staging & production (star-schema) schemas
```


### Staging & production schemas
![Schema](https://github.com/abreufreire/sparkify-postgres/blob/master/graphics/stag_prod_tables.png)


### Redshift tables description:
**Design:** 

The data distribution style for the tables has been set to DISTSTYLE = AUTO so Redshift assigns an optimal 
distribution style based on the size of the table data.

**Keys:** 

DISTKEY: choice was made considering the column that appears in most/bigger JOINs. The value in the DISTKEY column 
is hashed and values are distributed based on the hash (this value is used to distribute the data over any available 
slices). 

SORTKEY: choice was made considering the column more useful to sort rows in each slice; therefore the primary key of 
the *dimension tables* was used as SORTKEY. 

In the *fact table* the DISTKEY and SORTKEY were set to be the same join-column so the query optimizer/Redshift uses 
a sort merge join instead of a slower hash join.


### How to run
Clone this project, and to up and running it **locally** go to its directory (local machine) and create a virtual 
environment (venv), activate it and install the requirements.txt.

```
$ python3 -m venv /path/to/new/venv

$ source venv/bin/activate

$ pip install -r requirements.txt
```

Using AWS services create an IAM user (with administrator privileges) and add its security credential values (ACCESS & 
SECRET KEYS) to the Redshift configuration file (dwh.cfg).

- To create the pipeline:

```
$ python3 setup_cluster.py

$ python3 create_tables.py

$ python3 etl.py
```

- Verify/test the results (**analytics dashboard**) using jupyter notebook:

```
$ jupyter notebook  # launches app/browser/localhost with the project files
```

- Navigate to analytics.ipynb and run/analyze it.

- Clean up AWS services (when job is done):

```
$ python3 clean_redshift.py
```

### Data & metadata source 
<sup>1</sup>[Million Song Dataset](http://millionsongdataset.com/)
